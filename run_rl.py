import gym
from stable_baselines3 import DQN, PPO, A2C
from tool_env_dqn import ToolEnv
import sys

print(sys.argv)

if len(sys.argv) < 2:
    print("Invalid args. Provide ip and type of model")

ip = sys.argv[1]
model_type = int(sys.argv[2])

env = ToolEnv(ip, training=False)

if model_type == 1:
    model = DQN.load("./trained_models/ghostcat_vuln/dqn_tool_agent.zip", env=env)
elif model_type == 2:
    model = PPO.load("./trained_models/others/ppo_tool_agent.zip", env=env)
else:
    model = A2C.load("./trained_models/others/a2c_tool_agent.zip", env=env)

vec_env = model.get_env()

no_of_tools = 9
seen_tools = []
obs = vec_env.reset()


def isArrDiff(arr1, arr2):
    for i in range(len(arr1)):
        if arr1[i] != arr2[i]:
            return True
    return False

while True:
    print()
    print(f"Observation: {obs}")
    successful_tools = obs["agent"][0]
    print(successful_tools)

    action, _states = model.predict(obs, deterministic=True)
    print(f"Suggested Action: {action}")
    tool_action = action[0]

    print(f"Seen tools: {seen_tools}")

    # If that tool is already chosen
    if tool_action in seen_tools or successful_tools[tool_action] == 1:
        for i in range(0, no_of_tools):
            if i not in seen_tools and successful_tools[i] == 0:
                action = [i]
                break
    print(f"Accepted Action: {action}")
    tool_action = action[0]
    seen_tools.append(tool_action)

    next_obs, rewards, dones, info = vec_env.step(action)
    print(obs, next_obs, rewards, dones, seen_tools)

    # If goal reached
    if dones[0] is True or len(seen_tools) >= no_of_tools or sum(next_obs["agent"][0]) >= no_of_tools:
        break

    # If new state -> reset seen_tools
    if isArrDiff(obs['agent'][0], next_obs['agent'][0]):
        seen_tools.clear()
        arr = next_obs["agent"][0]
        for i in range(len(arr)):
            if(arr[i] == 1):
                seen_tools.append(i)


    obs = next_obs
    print()

vec_env.close()